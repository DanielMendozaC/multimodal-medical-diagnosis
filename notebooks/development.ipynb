{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal Medical Diagnosis System - Data Preparation\n",
    "# MVP for Pneumonia Detection using X-rays and Medical Transcriptions\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import random\n",
    "import re\n",
    "import traceback\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[logging.FileHandler(\"preprocessing.log\"),\n",
    "                              logging.StreamHandler()])\n",
    "\n",
    "# For text processing - with necessary NLTK downloads\n",
    "import nltk\n",
    "# Download needed NLTK packages first!\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "except Exception as e:\n",
    "    logging.warning(f\"NLTK download warning: {e}\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# For image processing\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "\n",
    "# For model preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Define paths\n",
    "base_path = 'data'\n",
    "chest_xray_path = os.path.join(base_path, 'chest_xray')\n",
    "mtsample_path = os.path.join(base_path, 'mtsamples.csv')\n",
    "\n",
    "logging.info(\"Notebook configuration complete!\")\n",
    "\n",
    "# 1. Image Data Processing - Using existing structure\n",
    "\n",
    "def is_valid_image(image_path):\n",
    "    \"\"\"Check if an image is valid and can be opened.\"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            # Just test if it opens\n",
    "            pass\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Invalid image {image_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_existing_xray_structure(base_path):\n",
    "    \"\"\"Process X-ray images with existing train/test/val directories.\"\"\"\n",
    "    dataset_info = {}\n",
    "    \n",
    "    # Process each split (train, test, val)\n",
    "    for split in ['train', 'test', 'val']:\n",
    "        split_path = os.path.join(base_path, split)\n",
    "        if not os.path.exists(split_path):\n",
    "            logging.warning(f\"Warning: {split} directory not found\")\n",
    "            continue\n",
    "        \n",
    "        normal_path = os.path.join(split_path, 'NORMAL')\n",
    "        pneumonia_path = os.path.join(split_path, 'PNEUMONIA')\n",
    "        \n",
    "        # Count images in each class\n",
    "        if os.path.exists(normal_path):\n",
    "            normal_images = [f for f in os.listdir(normal_path) \n",
    "                            if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        else:\n",
    "            normal_images = []\n",
    "            \n",
    "        if os.path.exists(pneumonia_path):\n",
    "            pneumonia_images = [f for f in os.listdir(pneumonia_path) \n",
    "                              if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        else:\n",
    "            pneumonia_images = []\n",
    "        \n",
    "        # Store paths and labels (check image validity)\n",
    "        normal_paths = [os.path.join(normal_path, img) for img in normal_images]\n",
    "        pneumonia_paths = [os.path.join(pneumonia_path, img) for img in pneumonia_images]\n",
    "        \n",
    "        # Verify image paths actually work\n",
    "        valid_normal_paths = [path for path in normal_paths if is_valid_image(path)]\n",
    "        valid_pneumonia_paths = [path for path in pneumonia_paths if is_valid_image(path)]\n",
    "        \n",
    "        all_paths = valid_normal_paths + valid_pneumonia_paths\n",
    "        all_labels = [0] * len(valid_normal_paths) + [1] * len(valid_pneumonia_paths)\n",
    "        \n",
    "        # Only create dataset info if we have valid images\n",
    "        if len(all_paths) > 0:\n",
    "            dataset_info[split] = {\n",
    "                'paths': all_paths,\n",
    "                'labels': all_labels,\n",
    "                'normal_count': len(valid_normal_paths),\n",
    "                'pneumonia_count': len(valid_pneumonia_paths)\n",
    "            }\n",
    "            \n",
    "            logging.info(f\"{split.capitalize()} set:\")\n",
    "            logging.info(f\"  NORMAL: {len(valid_normal_paths)} valid images (of {len(normal_images)} total)\")\n",
    "            logging.info(f\"  PNEUMONIA: {len(valid_pneumonia_paths)} valid images (of {len(pneumonia_images)} total)\")\n",
    "        else:\n",
    "            logging.warning(f\"No valid images found in {split} set\")\n",
    "    \n",
    "    return dataset_info\n",
    "\n",
    "# Create a function to load and preprocess images\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"Load and preprocess an image for deep learning models.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img = img.resize(target_size)\n",
    "        img_array = np.array(img) / 255.0  # Normalize to [0,1]\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Visualize sample images from each class\n",
    "def visualize_samples(dataset_info, split='train', num_samples=3):\n",
    "    \"\"\"Visualize sample images from each class.\"\"\"\n",
    "    if split not in dataset_info:\n",
    "        logging.warning(f\"Split '{split}' not found in dataset\")\n",
    "        return\n",
    "    \n",
    "    paths = dataset_info[split]['paths']\n",
    "    labels = dataset_info[split]['labels']\n",
    "    \n",
    "    pneumonia_indices = [i for i, label in enumerate(labels) if label == 1]\n",
    "    normal_indices = [i for i, label in enumerate(labels) if label == 0]\n",
    "    \n",
    "    if len(pneumonia_indices) == 0 or len(normal_indices) == 0:\n",
    "        logging.warning(f\"Not enough samples to visualize in {split} set\")\n",
    "        return\n",
    "    \n",
    "    # Get more samples than needed to account for potential failures\n",
    "    max_samples = min(num_samples * 3, len(pneumonia_indices))\n",
    "    pneumonia_samples = random.sample(pneumonia_indices, max_samples)\n",
    "    \n",
    "    max_samples = min(num_samples * 3, len(normal_indices))\n",
    "    normal_samples = random.sample(normal_indices, max_samples)\n",
    "    \n",
    "    # Prepare valid images\n",
    "    normal_imgs = []\n",
    "    for idx in normal_samples:\n",
    "        img = preprocess_image(paths[idx])\n",
    "        if img is not None:\n",
    "            normal_imgs.append((idx, img))\n",
    "        if len(normal_imgs) >= num_samples:\n",
    "            break\n",
    "            \n",
    "    pneumonia_imgs = []\n",
    "    for idx in pneumonia_samples:\n",
    "        img = preprocess_image(paths[idx])\n",
    "        if img is not None:\n",
    "            pneumonia_imgs.append((idx, img))\n",
    "        if len(pneumonia_imgs) >= num_samples:\n",
    "            break\n",
    "    \n",
    "    # Check if we have enough valid images\n",
    "    if len(normal_imgs) == 0 or len(pneumonia_imgs) == 0:\n",
    "        logging.warning(\"Not enough valid images to visualize\")\n",
    "        return\n",
    "        \n",
    "    # Use the minimum number available for both classes\n",
    "    n_samples = min(len(normal_imgs), len(pneumonia_imgs), num_samples)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_samples, figsize=(15, 8))\n",
    "    \n",
    "    # Handle case where n_samples is 1\n",
    "    if n_samples == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        if i < len(normal_imgs):\n",
    "            _, img = normal_imgs[i]\n",
    "            axes[0, i].imshow(img)\n",
    "            axes[0, i].set_title('Normal')\n",
    "            axes[0, i].axis('off')\n",
    "        \n",
    "        if i < len(pneumonia_imgs):\n",
    "            _, img = pneumonia_imgs[i]\n",
    "            axes[1, i].imshow(img)\n",
    "            axes[1, i].set_title('Pneumonia')\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sample_images.png')  # Save the figure\n",
    "    logging.info(\"Sample visualization saved to sample_images.png\")\n",
    "    plt.close()  # Close to avoid display issues\n",
    "\n",
    "# Process the image data\n",
    "logging.info(\"Processing X-ray image data:\")\n",
    "try:\n",
    "    image_data = process_existing_xray_structure(chest_xray_path)\n",
    "    \n",
    "    # Verify we have data\n",
    "    if not image_data:\n",
    "        logging.error(\"No valid image data found in the dataset\")\n",
    "    else:\n",
    "        # Visualize sample images\n",
    "        logging.info(\"Visualizing sample X-ray images:\")\n",
    "        try:\n",
    "            visualize_samples(image_data)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error visualizing samples: {e}\")\n",
    "            traceback.print_exc()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error processing image data: {e}\")\n",
    "    traceback.print_exc()\n",
    "    # Initialize with empty dict to allow rest of script to run\n",
    "    image_data = {}\n",
    "\n",
    "# 2. Text Data Processing\n",
    "\n",
    "# Load the MTSamples dataset\n",
    "def load_mtsamples():\n",
    "    \"\"\"Load and display info about the MTSamples dataset.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(mtsample_path)\n",
    "        logging.info(f\"Dataset shape: {df.shape}\")\n",
    "        logging.info(\"\\nColumns:\")\n",
    "        for col in df.columns:\n",
    "            logging.info(f\"- {col}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading MTSamples: {e}\")\n",
    "        # Return empty DataFrame to allow rest of script to run\n",
    "        return pd.DataFrame()\n",
    "\n",
    "logging.info(\"\\nLoading MTSamples dataset:\")\n",
    "mtsamples_df = load_mtsamples()\n",
    "\n",
    "# Helper function to find pneumonia/respiratory related samples\n",
    "def find_relevant_samples(df, keywords, column='transcription'):\n",
    "    \"\"\"Find samples containing relevant keywords.\"\"\"\n",
    "    if df.empty or column not in df.columns:\n",
    "        logging.warning(f\"Cannot find relevant samples: DataFrame is empty or missing '{column}' column\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    pattern = '|'.join(keywords)\n",
    "    try:\n",
    "        mask = df[column].str.contains(pattern, case=False, na=False)\n",
    "        return df[mask]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error finding relevant samples: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Define relevant keywords for pneumonia\n",
    "pneumonia_keywords = [\n",
    "    'pneumonia', 'lung infection', 'chest infection', 'respiratory infection',\n",
    "    'infiltrate', 'consolidation', 'pulmonary', 'respiratory'\n",
    "]\n",
    "\n",
    "# Filter relevant samples\n",
    "logging.info(\"\\nFiltering for pneumonia-related text data:\")\n",
    "try:\n",
    "    relevant_samples = find_relevant_samples(mtsamples_df, pneumonia_keywords)\n",
    "    logging.info(f\"Number of relevant samples found: {len(relevant_samples)}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error filtering text data: {e}\")\n",
    "    relevant_samples = pd.DataFrame()\n",
    "\n",
    "# Preprocess text data\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Text preprocessing with error handling.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error during text preprocessing: {e}\")\n",
    "        # Return simplified version as fallback\n",
    "        if isinstance(text, str):\n",
    "            return ' '.join(text.lower().split())\n",
    "        return \"\"\n",
    "\n",
    "# Apply preprocessing to all relevant samples\n",
    "logging.info(\"\\nPreprocessing text data:\")\n",
    "if not relevant_samples.empty and 'transcription' in relevant_samples.columns:\n",
    "    try:\n",
    "        relevant_samples['processed_text'] = relevant_samples['transcription'].apply(preprocess_text)\n",
    "        \n",
    "        # Example text preprocessing\n",
    "        if len(relevant_samples) > 0:\n",
    "            sample_text = relevant_samples.iloc[0]['transcription']\n",
    "            logging.info(\"\\nOriginal text sample:\")\n",
    "            if isinstance(sample_text, str) and len(sample_text) > 0:\n",
    "                logging.info(sample_text[:500] + \"...\")\n",
    "            \n",
    "            processed_text = preprocess_text(sample_text)\n",
    "            logging.info(\"\\nProcessed text sample:\")\n",
    "            logging.info(processed_text[:500] + \"...\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during text preprocessing: {e}\")\n",
    "else:\n",
    "    logging.warning(\"No text data to preprocess\")\n",
    "\n",
    "# 3. Save Processed Data\n",
    "def save_data_splits(image_data, text_data, output_dir='processed_data'):\n",
    "    \"\"\"Save the processed data for later use.\"\"\"\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save image paths and labels\n",
    "        for split_name, split_data in image_data.items():\n",
    "            split_dir = os.path.join(output_dir, split_name)\n",
    "            os.makedirs(split_dir, exist_ok=True)\n",
    "            \n",
    "            # Save image paths and labels\n",
    "            pd.DataFrame({\n",
    "                'image_path': split_data['paths'],\n",
    "                'label': split_data['labels']\n",
    "            }).to_csv(os.path.join(split_dir, 'image_data.csv'), index=False)\n",
    "        \n",
    "        # Save text data\n",
    "        if not text_data.empty:\n",
    "            text_data.to_csv(os.path.join(output_dir, 'processed_text.csv'), index=False)\n",
    "        \n",
    "        logging.info(f\"Data saved to {output_dir}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving data: {e}\")\n",
    "        return False\n",
    "\n",
    "# Save the processed data\n",
    "logging.info(\"\\nSaving processed data:\")\n",
    "save_result = save_data_splits(image_data, relevant_samples)\n",
    "\n",
    "# 4. Data Validation\n",
    "def validate_saved_data(output_dir='processed_data'):\n",
    "    \"\"\"Validate that saved data can be loaded properly.\"\"\"\n",
    "    try:\n",
    "        # Check if directory exists\n",
    "        if not os.path.exists(output_dir):\n",
    "            logging.warning(f\"Output directory {output_dir} not found\")\n",
    "            return False\n",
    "            \n",
    "        # Check image data\n",
    "        valid_splits = 0\n",
    "        for split in ['train', 'test', 'val']:\n",
    "            split_path = os.path.join(output_dir, split, 'image_data.csv')\n",
    "            if os.path.exists(split_path):\n",
    "                df = pd.read_csv(split_path)\n",
    "                logging.info(f\"Validated {split} data: {len(df)} records\")\n",
    "                valid_splits += 1\n",
    "        \n",
    "        # Check text data\n",
    "        text_path = os.path.join(output_dir, 'processed_text.csv')\n",
    "        if os.path.exists(text_path):\n",
    "            df = pd.read_csv(text_path)\n",
    "            logging.info(f\"Validated text data: {len(df)} records\")\n",
    "            \n",
    "        return valid_splits > 0\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error validating saved data: {e}\")\n",
    "        return False\n",
    "\n",
    "# Validate saved data\n",
    "if save_result:\n",
    "    logging.info(\"\\nValidating saved data:\")\n",
    "    validation_result = validate_saved_data()\n",
    "    if validation_result:\n",
    "        logging.info(\"Data validation successful\")\n",
    "    else:\n",
    "        logging.warning(\"Data validation failed or incomplete\")\n",
    "\n",
    "# 5. Summary and Next Steps\n",
    "logging.info(\"\\nData Preparation Summary:\")\n",
    "logging.info(\"1. Processed X-ray images with robust error handling\")\n",
    "logging.info(\"2. Identified relevant text samples from MTSamples\")\n",
    "logging.info(\"3. Saved processed data for model training\")\n",
    "\n",
    "logging.info(\"\\nNext Steps:\")\n",
    "logging.info(\"1. Train baseline models for each modality\")\n",
    "logging.info(\"2. Design fusion architecture\")\n",
    "logging.info(\"3. Evaluate combined model performance\")\n",
    "\n",
    "# Final status report\n",
    "if len(image_data) > 0 and not relevant_samples.empty:\n",
    "    logging.info(\"\\nData preparation completed successfully!\")\n",
    "else:\n",
    "    logging.warning(\"\\nData preparation completed with issues. Check logs for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X-ray Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pneumonia Detection - PyTorch X-ray Model\n",
    "# Building a CNN model for pneumonia detection from chest X-rays using PyTorch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to the processed data\n",
    "BASE_PATH = 'processed_data'\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# 1. Define the dataset class\n",
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"Dataset for chest X-ray images.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the csv file with image paths and labels.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data_frame.iloc[idx, 0]  # first column contains image paths\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        label = self.data_frame.iloc[idx, 1]  # second column contains labels\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "class RobustChestXrayDataset(Dataset):\n",
    "    \"\"\"Dataset for chest X-ray images with robustness against corrupted images.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None, fallback_image=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the csv file with image paths and labels.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            fallback_image (str, optional): Path to a fallback image to use for corrupted images.\n",
    "                If None, will use a random valid image from the dataset.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.fallback_image = fallback_image\n",
    "        self.valid_indices = list(range(len(self.data_frame)))\n",
    "        self.corrupted_images = set()\n",
    "        \n",
    "        # Check for corrupted images during initialization\n",
    "        self._validate_images()\n",
    "        \n",
    "        # Print dataset statistics\n",
    "        print(f\"Dataset initialized with {len(self.valid_indices)} valid images.\")\n",
    "        if self.corrupted_images:\n",
    "            print(f\"Found {len(self.corrupted_images)} corrupted images.\")\n",
    "    \n",
    "    def _validate_images(self):\n",
    "        \"\"\"Validate all images in the dataset and identify corrupted ones.\"\"\"\n",
    "        valid_images = []\n",
    "        self.corrupted_images = set()\n",
    "        \n",
    "        for idx in range(len(self.data_frame)):\n",
    "            img_path = self.data_frame.iloc[idx, 0]\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img.verify()  # Verify image is not corrupted\n",
    "                valid_images.append(idx)\n",
    "            except (UnidentifiedImageError, OSError, IOError) as e:\n",
    "                self.corrupted_images.add(idx)\n",
    "                logging.warning(f\"Corrupted image at index {idx}: {img_path}, Error: {e}\")\n",
    "        \n",
    "        self.valid_indices = valid_images\n",
    "        \n",
    "        # If no fallback image is provided, use the first valid image\n",
    "        if not self.fallback_image and valid_images:\n",
    "            self.fallback_image = self.data_frame.iloc[valid_images[0], 0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of valid images in the dataset.\"\"\"\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a sample from the dataset.\"\"\"\n",
    "        # Map the filtered index to the original index\n",
    "        mapped_idx = self.valid_indices[idx]\n",
    "        \n",
    "        img_path = self.data_frame.iloc[mapped_idx, 0]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except (UnidentifiedImageError, OSError, IOError) as e:\n",
    "            # This should not happen since we filtered out corrupted images\n",
    "            # But just in case, use the fallback image\n",
    "            logging.error(f\"Failed to load supposedly valid image: {img_path}, Error: {e}\")\n",
    "            if self.fallback_image:\n",
    "                image = Image.open(self.fallback_image).convert('RGB')\n",
    "            else:\n",
    "                # Create a blank image as last resort\n",
    "                image = Image.new('RGB', (224, 224), color=(128, 128, 128))\n",
    "        \n",
    "        label = self.data_frame.iloc[mapped_idx, 1]  # second column contains labels\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "# 2. Define data transformations\n",
    "def get_transforms():\n",
    "    \"\"\"Define data transformations for training and validation/testing.\"\"\"\n",
    "    # Training transformations with augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Validation/test transformations (no augmentation)\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "# 3. Create dataloaders\n",
    "def create_dataloaders():\n",
    "    \"\"\"Create PyTorch dataloaders for training, validation, and testing.\"\"\"\n",
    "    train_transform, val_transform = get_transforms()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ChestXrayDataset(\n",
    "        csv_file=os.path.join(BASE_PATH, 'train', 'image_data.csv'),\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = ChestXrayDataset(\n",
    "        csv_file=os.path.join(BASE_PATH, 'val', 'image_data.csv'),\n",
    "        transform=val_transform\n",
    "    )\n",
    "    \n",
    "    test_dataset = ChestXrayDataset(\n",
    "        csv_file=os.path.join(BASE_PATH, 'test', 'image_data.csv'),\n",
    "        transform=val_transform\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    # Setting num_workers=0 to avoid the multiprocessing issue\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Print dataset information\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# 4. Define the model using transfer learning\n",
    "class PneumoniaClassifier(nn.Module):\n",
    "    \"\"\"CNN model for pneumonia classification using transfer learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_name='densenet121', pretrained=True, freeze_base=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_model_name (str): Name of the base model to use.\n",
    "            pretrained (bool): Whether to use pretrained weights.\n",
    "            freeze_base (bool): Whether to freeze the base model layers.\n",
    "        \"\"\"\n",
    "        super(PneumoniaClassifier, self).__init__()\n",
    "        \n",
    "        # Load the base model\n",
    "        if base_model_name == 'densenet121':\n",
    "            self.base_model = models.densenet121(weights='IMAGENET1K_V1' if pretrained else None)\n",
    "            num_ftrs = self.base_model.classifier.in_features\n",
    "            self.base_model.classifier = nn.Identity()\n",
    "        elif base_model_name == 'resnet50':\n",
    "            self.base_model = models.resnet50(weights='IMAGENET1K_V1' if pretrained else None)\n",
    "            num_ftrs = self.base_model.fc.in_features\n",
    "            self.base_model.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported base model: {base_model_name}\")\n",
    "        \n",
    "        # Freeze the base model if requested\n",
    "        if freeze_base:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Custom classifier for pneumonia detection\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        self.base_model_name = base_model_name\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        features = self.base_model(x)\n",
    "        return self.classifier(features)\n",
    "    \n",
    "    def unfreeze_layers(self, num_layers=10):\n",
    "        \"\"\"Unfreeze the last num_layers of the base model for fine-tuning.\"\"\"\n",
    "        if self.base_model_name == 'densenet121':\n",
    "            layers = list(self.base_model.features.children())\n",
    "            for layer in layers[-num_layers:]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "        elif self.base_model_name == 'resnet50':\n",
    "            layers = list(self.base_model.children())\n",
    "            for layer in layers[-num_layers:]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "# 5. Define training and validation functions\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = torch.sigmoid(outputs) >= 0.5\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\"loss\": loss.item(), \"acc\": correct/total})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = torch.sigmoid(outputs) >= 0.5\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Store predictions and labels for metrics\n",
    "            all_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    # Calculate AUC\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    return epoch_loss, epoch_acc, roc_auc, all_preds, all_labels\n",
    "\n",
    "# 6. Training loop with early stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, \n",
    "                num_epochs=NUM_EPOCHS, patience=5):\n",
    "    \"\"\"Train and validate the model with early stopping.\"\"\"\n",
    "    best_val_auc = 0.0\n",
    "    best_model_wts = None\n",
    "    no_improve_epochs = 0\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_auc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc, val_auc, _, _ = validate(model, val_loader, criterion, device)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "        \n",
    "        # Step the scheduler if provided\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        # Check if this is the best model so far\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            no_improve_epochs = 0\n",
    "            print(f\"New best model with Val AUC: {val_auc:.4f}\")\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            print(f\"No improvement for {no_improve_epochs} epochs\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Main execution code\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = create_dataloaders()\n",
    "\n",
    "    # Initialize the model\n",
    "    model = PneumoniaClassifier(base_model_name='densenet121', pretrained=True, freeze_base=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model (first stage with frozen base layers)\n",
    "    print(\"Training Stage 1: Frozen base model\")\n",
    "    model, history_stage1 = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, None, num_epochs=10, patience=5\n",
    "    )\n",
    "\n",
    "    # Fine-tuning stage (unfreeze some layers)\n",
    "    print(\"\\nTraining Stage 2: Fine-tuning with unfrozen layers\")\n",
    "    model.unfreeze_layers(num_layers=10)  # Unfreeze the last 10 layers\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "\n",
    "    model, history_stage2 = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, None, num_epochs=10, patience=5\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'pneumonia_classifier.pth')\n",
    "    print(\"Model saved to pneumonia_classifier.pth\")\n",
    "\n",
    "    # 8. Plot training history\n",
    "    def plot_training_history(history_stage1, history_stage2=None):\n",
    "        \"\"\"Plot the training and validation metrics.\"\"\"\n",
    "        # Combine histories if we have two stages\n",
    "        if history_stage2:\n",
    "            train_loss = history_stage1['train_loss'] + history_stage2['train_loss']\n",
    "            val_loss = history_stage1['val_loss'] + history_stage2['val_loss']\n",
    "            train_acc = history_stage1['train_acc'] + history_stage2['train_acc']\n",
    "            val_acc = history_stage1['val_acc'] + history_stage2['val_acc']\n",
    "            val_auc = history_stage1['val_auc'] + history_stage2['val_auc']\n",
    "            epochs = range(1, len(train_loss) + 1)\n",
    "            stage1_end = len(history_stage1['train_loss'])\n",
    "        else:\n",
    "            train_loss = history_stage1['train_loss']\n",
    "            val_loss = history_stage1['val_loss']\n",
    "            train_acc = history_stage1['train_acc']\n",
    "            val_acc = history_stage1['val_acc']\n",
    "            val_auc = history_stage1['val_auc']\n",
    "            epochs = range(1, len(train_loss) + 1)\n",
    "            stage1_end = len(train_loss)\n",
    "        \n",
    "        # Create figure with 2 subplots\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
    "        plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "        if history_stage2:\n",
    "            plt.axvline(x=stage1_end, color='g', linestyle='--', label='Start Fine-tuning')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(epochs, train_acc, 'b-', label='Training Accuracy')\n",
    "        plt.plot(epochs, val_acc, 'r-', label='Validation Accuracy')\n",
    "        if history_stage2:\n",
    "            plt.axvline(x=stage1_end, color='g', linestyle='--', label='Start Fine-tuning')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot AUC\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(epochs, val_auc, 'r-', label='Validation AUC')\n",
    "        if history_stage2:\n",
    "            plt.axvline(x=stage1_end, color='g', linestyle='--', label='Start Fine-tuning')\n",
    "        plt.title('Validation AUC')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(history_stage1, history_stage2)\n",
    "\n",
    "    # 9. Evaluate on the test set\n",
    "    def evaluate_model(model, test_loader, criterion, device):\n",
    "        \"\"\"Evaluate the model on the test set.\"\"\"\n",
    "        # Get test predictions\n",
    "        test_loss, test_acc, test_auc, test_preds, test_labels = validate(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nTest Results:\")\n",
    "        print(f\"Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, AUC: {test_auc:.4f}\")\n",
    "        \n",
    "        # Convert predictions to binary\n",
    "        test_preds_binary = (np.array(test_preds) >= 0.5).astype(int)\n",
    "        \n",
    "        # Print classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(test_labels, test_preds_binary, target_names=['Normal', 'Pneumonia']))\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        cm = confusion_matrix(test_labels, test_preds_binary)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(\n",
    "            cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Pneumonia'], \n",
    "            yticklabels=['Normal', 'Pneumonia']\n",
    "        )\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        fpr, tpr, _ = roc_curve(test_labels, test_preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'loss': test_loss,\n",
    "            'accuracy': test_acc,\n",
    "            'auc': test_auc,\n",
    "            'predictions': test_preds,\n",
    "            'labels': test_labels\n",
    "        }\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_results = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "    # 10. Visualize some predictions\n",
    "    def visualize_predictions(model, test_dataset, num_samples=5):\n",
    "        \"\"\"Visualize some example predictions.\"\"\"\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Randomly select samples\n",
    "        indices = random.sample(range(len(test_dataset)), num_samples)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
    "        \n",
    "        # Get the inverse transform to display images\n",
    "        inv_normalize = transforms.Compose([\n",
    "            transforms.Normalize(\n",
    "                mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "                std=[1/0.229, 1/0.224, 1/0.225]\n",
    "            ),\n",
    "            transforms.ToPILImage()\n",
    "        ])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, idx in enumerate(indices):\n",
    "                # Get the image and label\n",
    "                image, label = test_dataset[idx]\n",
    "                image_tensor = image.unsqueeze(0).to(device)\n",
    "                \n",
    "                # Make prediction\n",
    "                output = model(image_tensor).squeeze()\n",
    "                pred_prob = torch.sigmoid(output).item()\n",
    "                pred_class = \"Pneumonia\" if pred_prob >= 0.5 else \"Normal\"\n",
    "                true_class = \"Pneumonia\" if label.item() == 1 else \"Normal\"\n",
    "                \n",
    "                # Color green for correct predictions, red for incorrect\n",
    "                color = \"green\" if pred_class == true_class else \"red\"\n",
    "                \n",
    "                # Convert tensor to image for display\n",
    "                img_to_show = inv_normalize(image.cpu())\n",
    "                \n",
    "                # Display the image and prediction\n",
    "                axes[i].imshow(img_to_show)\n",
    "                axes[i].set_title(f\"Pred: {pred_class}\\nTrue: {true_class}\\nProb: {pred_prob:.2f}\", color=color)\n",
    "                axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Visualize some predictions\n",
    "    visualize_predictions(model, test_dataset)\n",
    "\n",
    "    # 11. Extract and save features for multimodal fusion\n",
    "    def extract_features(model, dataloader, device):\n",
    "        \"\"\"Extract features from the model for the dataloader samples.\"\"\"\n",
    "        # Create a feature extractor model\n",
    "        if isinstance(model, PneumoniaClassifier):\n",
    "            # Create a copy of the model that outputs features before the final classification layer\n",
    "            class FeatureExtractor(nn.Module):\n",
    "                def __init__(self, model):\n",
    "                    super(FeatureExtractor, self).__init__()\n",
    "                    self.base_model = model.base_model\n",
    "                    \n",
    "                    # Get the feature extractor part (all but the last layer)\n",
    "                    classifier_layers = list(model.classifier.children())\n",
    "                    self.feature_layers = nn.Sequential(*classifier_layers[:-2])\n",
    "                \n",
    "                def forward(self, x):\n",
    "                    features = self.base_model(x)\n",
    "                    return self.feature_layers(features)\n",
    "            \n",
    "            feature_extractor = FeatureExtractor(model).to(device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model type for feature extraction\")\n",
    "        \n",
    "        # Extract features\n",
    "        feature_extractor.eval()\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, batch_labels in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "                inputs = inputs.to(device)\n",
    "                batch_features = feature_extractor(inputs)\n",
    "                features.append(batch_features.cpu().numpy())\n",
    "                labels.append(batch_labels.numpy())\n",
    "        \n",
    "        # Concatenate batches\n",
    "        features = np.vstack(features)\n",
    "        labels = np.concatenate(labels)\n",
    "        \n",
    "        return features, labels\n",
    "\n",
    "    # Extract features for all sets\n",
    "    print(\"\\nExtracting features for future multimodal fusion:\")\n",
    "    train_features, train_labels = extract_features(model, train_loader, device)\n",
    "    val_features, val_labels = extract_features(model, val_loader, device)\n",
    "    test_features, test_labels = extract_features(model, test_loader, device)\n",
    "\n",
    "    # Save features\n",
    "    features_path = 'image_features.npz'\n",
    "    np.savez(\n",
    "        features_path,\n",
    "        train_features=train_features, train_labels=train_labels,\n",
    "        val_features=val_features, val_labels=val_labels,\n",
    "        test_features=test_features, test_labels=test_labels\n",
    "    )\n",
    "    print(f\"Features saved to {features_path}\")\n",
    "\n",
    "    # 12. Summary and conclusion\n",
    "    print(\"\\nImage Model Summary:\")\n",
    "    print(f\"Architecture: DenseNet121 with custom classifier\")\n",
    "    print(f\"Training strategy: Two-stage training with transfer learning\")\n",
    "    print(f\"Test Accuracy: {test_results['accuracy']:.4f}\")\n",
    "    print(f\"Test AUC: {test_results['auc']:.4f}\")\n",
    "\n",
    "    print(\"\\nNext Steps:\")\n",
    "    print(\"1. Build a text model to extract key pneumonia indicators from medical text\")\n",
    "    print(\"2. Create a visualization dashboard showing X-ray predictions and text insights\")\n",
    "    print(\"3. Demonstrate how integrating both modalities improves diagnostic capability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders():\n",
    "    train_csv_path = os.path.join(BASE_PATH, 'train', 'image_data.csv')\n",
    "    val_csv_path = os.path.join(BASE_PATH, 'val', 'image_data.csv')\n",
    "    test_csv_path = os.path.join(BASE_PATH, 'test', 'image_data.csv')\n",
    "    \n",
    "    print(f\"Checking if train CSV exists: {os.path.exists(train_csv_path)}\")\n",
    "    if os.path.exists(train_csv_path):\n",
    "        df = pd.read_csv(train_csv_path)\n",
    "        print(f\"Train CSV shape: {df.shape}\")\n",
    "        print(f\"Train CSV first few rows:\\n{df.head()}\")\n",
    "    \n",
    "    # Rest of the function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pneumonia Detection - Error Handling for Corrupted Images\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import shutil\n",
    "\n",
    "def identify_corrupted_images(csv_file):\n",
    "    \"\"\"Check all images in the CSV file and identify any corrupted ones.\"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    corrupted_images = []\n",
    "    \n",
    "    print(f\"Checking {len(df)} images for corruption...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        img_path = row['image_path']\n",
    "        try:\n",
    "            # Try to open the image\n",
    "            with Image.open(img_path) as img:\n",
    "                # Try to load the image data\n",
    "                img.load()\n",
    "        except (UnidentifiedImageError, OSError, IOError) as e:\n",
    "            print(f\"Corrupted image found: {img_path}\")\n",
    "            print(f\"Error: {e}\")\n",
    "            corrupted_images.append(img_path)\n",
    "    \n",
    "    return corrupted_images\n",
    "\n",
    "def remove_corrupted_from_csv(csv_file, corrupted_images, output_file=None):\n",
    "    \"\"\"Remove corrupted images from the CSV file.\"\"\"\n",
    "    if not output_file:\n",
    "        output_file = csv_file.replace('.csv', '_clean.csv')\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Filter out corrupted images\n",
    "    clean_df = df[~df['image_path'].isin(corrupted_images)]\n",
    "    \n",
    "    # Save the clean dataframe\n",
    "    clean_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Removed {len(corrupted_images)} corrupted images from CSV.\")\n",
    "    print(f\"Original CSV had {len(df)} entries, new CSV has {len(clean_df)} entries.\")\n",
    "    print(f\"Clean CSV saved to {output_file}\")\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "def check_and_clean_dataset():\n",
    "    \"\"\"Check all splits of the dataset and clean them.\"\"\"\n",
    "    base_path = 'processed_data'\n",
    "    splits = ['train', 'val', 'test']\n",
    "    \n",
    "    for split in splits:\n",
    "        csv_path = os.path.join(base_path, split, 'image_data.csv')\n",
    "        \n",
    "        if os.path.exists(csv_path):\n",
    "            print(f\"\\nChecking {split} split...\")\n",
    "            corrupted = identify_corrupted_images(csv_path)\n",
    "            \n",
    "            if corrupted:\n",
    "                # Make backup of original CSV\n",
    "                backup_path = os.path.join(base_path, split, f'image_data_backup.csv')\n",
    "                shutil.copy(csv_path, backup_path)\n",
    "                print(f\"Backup created at {backup_path}\")\n",
    "                \n",
    "                # Remove corrupted images from CSV\n",
    "                remove_corrupted_from_csv(csv_path, corrupted, csv_path)\n",
    "            else:\n",
    "                print(f\"No corrupted images found in {split} split.\")\n",
    "        else:\n",
    "            print(f\"CSV file not found for {split} split: {csv_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_and_clean_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
