{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mttext = pd.read_csv('../data/mtsamples.csv')\n",
    "df_mttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mttext[df_mttext['description'].str.contains('pneumonia')].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mttext.medical_specialty.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal Medical Diagnosis System - Data Preparation\n",
    "# MVP for Pneumonia Detection using X-rays and Medical Transcriptions\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import random\n",
    "import re\n",
    "\n",
    "# For text processing - with necessary NLTK downloads\n",
    "import nltk\n",
    "# Download needed NLTK packages first!\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# For image processing\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "\n",
    "# For model preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Define paths\n",
    "base_path = 'data'\n",
    "chest_xray_path = os.path.join(base_path, 'chest_xray')\n",
    "mtsample_path = os.path.join(base_path, 'mtsamples.csv')\n",
    "\n",
    "print(\"Notebook configuration complete!\")\n",
    "\n",
    "# 1. Image Data Processing - Using existing structure\n",
    "\n",
    "def process_existing_xray_structure(base_path):\n",
    "    \"\"\"Process X-ray images with existing train/test/val directories.\"\"\"\n",
    "    dataset_info = {}\n",
    "    \n",
    "    # Process each split (train, test, val)\n",
    "    for split in ['train', 'test', 'val']:\n",
    "        split_path = os.path.join(base_path, split)\n",
    "        if not os.path.exists(split_path):\n",
    "            print(f\"Warning: {split} directory not found\")\n",
    "            continue\n",
    "        \n",
    "        normal_path = os.path.join(split_path, 'NORMAL')\n",
    "        pneumonia_path = os.path.join(split_path, 'PNEUMONIA')\n",
    "        \n",
    "        # Count images in each class\n",
    "        if os.path.exists(normal_path):\n",
    "            normal_images = [f for f in os.listdir(normal_path) \n",
    "                            if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        else:\n",
    "            normal_images = []\n",
    "            \n",
    "        if os.path.exists(pneumonia_path):\n",
    "            pneumonia_images = [f for f in os.listdir(pneumonia_path) \n",
    "                              if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        else:\n",
    "            pneumonia_images = []\n",
    "        \n",
    "        # Store paths and labels\n",
    "        normal_paths = [os.path.join(normal_path, img) for img in normal_images]\n",
    "        pneumonia_paths = [os.path.join(pneumonia_path, img) for img in pneumonia_images]\n",
    "        \n",
    "        all_paths = normal_paths + pneumonia_paths\n",
    "        all_labels = [0] * len(normal_paths) + [1] * len(pneumonia_paths)\n",
    "        \n",
    "        dataset_info[split] = {\n",
    "            'paths': all_paths,\n",
    "            'labels': all_labels,\n",
    "            'normal_count': len(normal_images),\n",
    "            'pneumonia_count': len(pneumonia_images)\n",
    "        }\n",
    "        \n",
    "        print(f\"{split.capitalize()} set:\")\n",
    "        print(f\"  NORMAL: {len(normal_images)} images\")\n",
    "        print(f\"  PNEUMONIA: {len(pneumonia_images)} images\")\n",
    "    \n",
    "    return dataset_info\n",
    "\n",
    "# Process the image data\n",
    "print(\"Processing X-ray image data:\")\n",
    "image_data = process_existing_xray_structure(chest_xray_path)\n",
    "\n",
    "# Create a function to load and preprocess images\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"Load and preprocess an image for deep learning models.\"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = img.resize(target_size)\n",
    "    img_array = np.array(img) / 255.0  # Normalize to [0,1]\n",
    "    return img_array\n",
    "\n",
    "# Visualize sample images from each class\n",
    "def visualize_samples(dataset_info, split='train', num_samples=3):\n",
    "    \"\"\"Visualize sample images from each class.\"\"\"\n",
    "    if split not in dataset_info:\n",
    "        print(f\"Split '{split}' not found in dataset\")\n",
    "        return\n",
    "    \n",
    "    paths = dataset_info[split]['paths']\n",
    "    labels = dataset_info[split]['labels']\n",
    "    \n",
    "    pneumonia_indices = [i for i, label in enumerate(labels) if label == 1]\n",
    "    normal_indices = [i for i, label in enumerate(labels) if label == 0]\n",
    "    \n",
    "    pneumonia_samples = random.sample(pneumonia_indices, min(num_samples, len(pneumonia_indices)))\n",
    "    normal_samples = random.sample(normal_indices, min(num_samples, len(normal_indices)))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 8))\n",
    "    \n",
    "    for i, idx in enumerate(normal_samples):\n",
    "        img = preprocess_image(paths[idx])\n",
    "        axes[0, i].imshow(img)\n",
    "        axes[0, i].set_title('Normal')\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "    for i, idx in enumerate(pneumonia_samples):\n",
    "        img = preprocess_image(paths[idx])\n",
    "        axes[1, i].imshow(img)\n",
    "        axes[1, i].set_title('Pneumonia')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize sample images\n",
    "print(\"\\nVisualizing sample X-ray images:\")\n",
    "visualize_samples(image_data)\n",
    "\n",
    "# 2. Text Data Processing\n",
    "\n",
    "# Load the MTSamples dataset\n",
    "def load_mtsamples():\n",
    "    \"\"\"Load and display info about the MTSamples dataset.\"\"\"\n",
    "    df = pd.read_csv(mtsample_path)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nColumns:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"- {col}\")\n",
    "    print(\"\\nSample data:\")\n",
    "    return df\n",
    "\n",
    "print(\"\\nLoading MTSamples dataset:\")\n",
    "mtsamples_df = load_mtsamples()\n",
    "print(mtsamples_df.head())\n",
    "\n",
    "# Helper function to find pneumonia/respiratory related samples\n",
    "def find_relevant_samples(df, keywords, column='transcription'):\n",
    "    \"\"\"Find samples containing relevant keywords.\"\"\"\n",
    "    pattern = '|'.join(keywords)\n",
    "    mask = df[column].str.contains(pattern, case=False, na=False)\n",
    "    return df[mask]\n",
    "\n",
    "# Define relevant keywords for pneumonia\n",
    "pneumonia_keywords = [\n",
    "    'pneumonia', 'lung infection', 'chest infection', 'respiratory infection',\n",
    "    'infiltrate', 'consolidation', 'pulmonary', 'respiratory'\n",
    "]\n",
    "\n",
    "# Filter relevant samples\n",
    "print(\"\\nFiltering for pneumonia-related text data:\")\n",
    "relevant_samples = find_relevant_samples(mtsamples_df, pneumonia_keywords)\n",
    "print(f\"Number of relevant samples found: {len(relevant_samples)}\")\n",
    "\n",
    "# Preprocess text data - simplified version without lemmatization\n",
    "def preprocess_text_simple(text):\n",
    "    \"\"\"Basic text preprocessing without using WordNet lemmatizer.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to all relevant samples\n",
    "print(\"\\nPreprocessing text data:\")\n",
    "relevant_samples['processed_text'] = relevant_samples['transcription'].apply(preprocess_text_simple)\n",
    "\n",
    "# Example text preprocessing\n",
    "if len(relevant_samples) > 0:\n",
    "    sample_text = relevant_samples.iloc[0]['transcription']\n",
    "    print(\"\\nOriginal text sample:\")\n",
    "    print(sample_text[:500], \"...\\n\")\n",
    "    \n",
    "    processed_text = preprocess_text_simple(sample_text)\n",
    "    print(\"Processed text sample:\")\n",
    "    print(processed_text[:500], \"...\")\n",
    "\n",
    "# 3. Save Processed Data\n",
    "def save_data_splits(image_data, text_data, output_dir='processed_data'):\n",
    "    \"\"\"Save the processed data for later use.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save image paths and labels\n",
    "    for split_name, split_data in image_data.items():\n",
    "        split_dir = os.path.join(output_dir, split_name)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        # Save image paths and labels\n",
    "        pd.DataFrame({\n",
    "            'image_path': split_data['paths'],\n",
    "            'label': split_data['labels']\n",
    "        }).to_csv(os.path.join(split_dir, 'image_data.csv'), index=False)\n",
    "    \n",
    "    # Save text data\n",
    "    text_data.to_csv(os.path.join(output_dir, 'processed_text.csv'), index=False)\n",
    "    \n",
    "    print(f\"Data saved to {output_dir}\")\n",
    "\n",
    "# Save the processed data\n",
    "print(\"\\nSaving processed data:\")\n",
    "save_data_splits(image_data, relevant_samples)\n",
    "\n",
    "# 4. Summary and Next Steps\n",
    "print(\"\\nData Preparation Summary:\")\n",
    "print(\"1. Processed X-ray images from existing train/val/test splits\")\n",
    "print(\"2. Identified relevant text samples from MTSamples\")\n",
    "print(\"3. Saved processed data for model training\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Train baseline models for each modality\")\n",
    "print(\"2. Design fusion architecture\")\n",
    "print(\"3. Evaluate combined model performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
