{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danielmendoza/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danielmendoza/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/danielmendoza/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook configuration complete!\n",
      "Processing X-ray image data:\n",
      "Train set:\n",
      "  NORMAL: 1341 images\n",
      "  PNEUMONIA: 3875 images\n",
      "Test set:\n",
      "  NORMAL: 234 images\n",
      "  PNEUMONIA: 390 images\n",
      "Val set:\n",
      "  NORMAL: 8 images\n",
      "  PNEUMONIA: 8 images\n",
      "\n",
      "Visualizing sample X-ray images:\n"
     ]
    },
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file 'data/chest_xray/train/NORMAL/NORMAL2-IM-0455-0001.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 139\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Visualize sample images\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mVisualizing sample X-ray images:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 139\u001b[0m \u001b[43mvisualize_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# 2. Text Data Processing\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Load the MTSamples dataset\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_mtsamples\u001b[39m():\n",
      "Cell \u001b[0;32mIn[9], line 123\u001b[0m, in \u001b[0;36mvisualize_samples\u001b[0;34m(dataset_info, split, num_samples)\u001b[0m\n\u001b[1;32m    120\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, num_samples, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(normal_samples):\n\u001b[0;32m--> 123\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     axes[\u001b[38;5;241m0\u001b[39m, i]\u001b[38;5;241m.\u001b[39mimshow(img)\n\u001b[1;32m    125\u001b[0m     axes[\u001b[38;5;241m0\u001b[39m, i]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 99\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[0;34m(image_path, target_size)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_image\u001b[39m(image_path, target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load and preprocess an image for deep learning models.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    100\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mresize(target_size)\n\u001b[1;32m    101\u001b[0m     img_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize to [0,1]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_pytorch/lib/python3.9/site-packages/PIL/Image.py:3532\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3530\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[1;32m   3531\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[0;32m-> 3532\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file 'data/chest_xray/train/NORMAL/NORMAL2-IM-0455-0001.jpeg'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Multimodal Medical Diagnosis System - Data Preparation\n",
    "# MVP for Pneumonia Detection using X-rays and Medical Transcriptions\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import random\n",
    "import re\n",
    "\n",
    "# For text processing - with necessary NLTK downloads\n",
    "import nltk\n",
    "# Download needed NLTK packages first!\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# For image processing\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "\n",
    "# For model preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Define paths\n",
    "base_path = 'data'\n",
    "chest_xray_path = os.path.join(base_path, 'chest_xray')\n",
    "mtsample_path = os.path.join(base_path, 'mtsamples.csv')\n",
    "\n",
    "print(\"Notebook configuration complete!\")\n",
    "\n",
    "# 1. Image Data Processing - Using existing structure\n",
    "\n",
    "def process_existing_xray_structure(base_path):\n",
    "    \"\"\"Process X-ray images with existing train/test/val directories.\"\"\"\n",
    "    dataset_info = {}\n",
    "    \n",
    "    # Process each split (train, test, val)\n",
    "    for split in ['train', 'test', 'val']:\n",
    "        split_path = os.path.join(base_path, split)\n",
    "        if not os.path.exists(split_path):\n",
    "            print(f\"Warning: {split} directory not found\")\n",
    "            continue\n",
    "        \n",
    "        normal_path = os.path.join(split_path, 'NORMAL')\n",
    "        pneumonia_path = os.path.join(split_path, 'PNEUMONIA')\n",
    "        \n",
    "        # Count images in each class\n",
    "        if os.path.exists(normal_path):\n",
    "            normal_images = [f for f in os.listdir(normal_path) \n",
    "                            if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        else:\n",
    "            normal_images = []\n",
    "            \n",
    "        if os.path.exists(pneumonia_path):\n",
    "            pneumonia_images = [f for f in os.listdir(pneumonia_path) \n",
    "                              if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        else:\n",
    "            pneumonia_images = []\n",
    "        \n",
    "        # Store paths and labels\n",
    "        normal_paths = [os.path.join(normal_path, img) for img in normal_images]\n",
    "        pneumonia_paths = [os.path.join(pneumonia_path, img) for img in pneumonia_images]\n",
    "        \n",
    "        all_paths = normal_paths + pneumonia_paths\n",
    "        all_labels = [0] * len(normal_paths) + [1] * len(pneumonia_paths)\n",
    "        \n",
    "        dataset_info[split] = {\n",
    "            'paths': all_paths,\n",
    "            'labels': all_labels,\n",
    "            'normal_count': len(normal_images),\n",
    "            'pneumonia_count': len(pneumonia_images)\n",
    "        }\n",
    "        \n",
    "        print(f\"{split.capitalize()} set:\")\n",
    "        print(f\"  NORMAL: {len(normal_images)} images\")\n",
    "        print(f\"  PNEUMONIA: {len(pneumonia_images)} images\")\n",
    "    \n",
    "    return dataset_info\n",
    "\n",
    "# Process the image data\n",
    "print(\"Processing X-ray image data:\")\n",
    "image_data = process_existing_xray_structure(chest_xray_path)\n",
    "\n",
    "# Create a function to load and preprocess images\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"Load and preprocess an image for deep learning models.\"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = img.resize(target_size)\n",
    "    img_array = np.array(img) / 255.0  # Normalize to [0,1]\n",
    "    return img_array\n",
    "\n",
    "# Visualize sample images from each class\n",
    "def visualize_samples(dataset_info, split='train', num_samples=3):\n",
    "    \"\"\"Visualize sample images from each class.\"\"\"\n",
    "    if split not in dataset_info:\n",
    "        print(f\"Split '{split}' not found in dataset\")\n",
    "        return\n",
    "    \n",
    "    paths = dataset_info[split]['paths']\n",
    "    labels = dataset_info[split]['labels']\n",
    "    \n",
    "    pneumonia_indices = [i for i, label in enumerate(labels) if label == 1]\n",
    "    normal_indices = [i for i, label in enumerate(labels) if label == 0]\n",
    "    \n",
    "    pneumonia_samples = random.sample(pneumonia_indices, min(num_samples, len(pneumonia_indices)))\n",
    "    normal_samples = random.sample(normal_indices, min(num_samples, len(normal_indices)))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 8))\n",
    "    \n",
    "    for i, idx in enumerate(normal_samples):\n",
    "        img = preprocess_image(paths[idx])\n",
    "        axes[0, i].imshow(img)\n",
    "        axes[0, i].set_title('Normal')\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "    for i, idx in enumerate(pneumonia_samples):\n",
    "        img = preprocess_image(paths[idx])\n",
    "        axes[1, i].imshow(img)\n",
    "        axes[1, i].set_title('Pneumonia')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize sample images\n",
    "print(\"\\nVisualizing sample X-ray images:\")\n",
    "visualize_samples(image_data)\n",
    "\n",
    "# 2. Text Data Processing\n",
    "\n",
    "# Load the MTSamples dataset\n",
    "def load_mtsamples():\n",
    "    \"\"\"Load and display info about the MTSamples dataset.\"\"\"\n",
    "    df = pd.read_csv(mtsample_path)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nColumns:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"- {col}\")\n",
    "    print(\"\\nSample data:\")\n",
    "    return df\n",
    "\n",
    "print(\"\\nLoading MTSamples dataset:\")\n",
    "mtsamples_df = load_mtsamples()\n",
    "print(mtsamples_df.head())\n",
    "\n",
    "# Helper function to find pneumonia/respiratory related samples\n",
    "def find_relevant_samples(df, keywords, column='transcription'):\n",
    "    \"\"\"Find samples containing relevant keywords.\"\"\"\n",
    "    pattern = '|'.join(keywords)\n",
    "    mask = df[column].str.contains(pattern, case=False, na=False)\n",
    "    return df[mask]\n",
    "\n",
    "# Define relevant keywords for pneumonia\n",
    "pneumonia_keywords = [\n",
    "    'pneumonia', 'lung infection', 'chest infection', 'respiratory infection',\n",
    "    'infiltrate', 'consolidation', 'pulmonary', 'respiratory'\n",
    "]\n",
    "\n",
    "# Filter relevant samples\n",
    "print(\"\\nFiltering for pneumonia-related text data:\")\n",
    "relevant_samples = find_relevant_samples(mtsamples_df, pneumonia_keywords)\n",
    "print(f\"Number of relevant samples found: {len(relevant_samples)}\")\n",
    "\n",
    "# Preprocess text data - simplified version without lemmatization\n",
    "def preprocess_text_simple(text):\n",
    "    \"\"\"Basic text preprocessing without using WordNet lemmatizer.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to all relevant samples\n",
    "print(\"\\nPreprocessing text data:\")\n",
    "relevant_samples['processed_text'] = relevant_samples['transcription'].apply(preprocess_text_simple)\n",
    "\n",
    "# Example text preprocessing\n",
    "if len(relevant_samples) > 0:\n",
    "    sample_text = relevant_samples.iloc[0]['transcription']\n",
    "    print(\"\\nOriginal text sample:\")\n",
    "    print(sample_text[:500], \"...\\n\")\n",
    "    \n",
    "    processed_text = preprocess_text_simple(sample_text)\n",
    "    print(\"Processed text sample:\")\n",
    "    print(processed_text[:500], \"...\")\n",
    "\n",
    "# 3. Save Processed Data\n",
    "def save_data_splits(image_data, text_data, output_dir='processed_data'):\n",
    "    \"\"\"Save the processed data for later use.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save image paths and labels\n",
    "    for split_name, split_data in image_data.items():\n",
    "        split_dir = os.path.join(output_dir, split_name)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        # Save image paths and labels\n",
    "        pd.DataFrame({\n",
    "            'image_path': split_data['paths'],\n",
    "            'label': split_data['labels']\n",
    "        }).to_csv(os.path.join(split_dir, 'image_data.csv'), index=False)\n",
    "    \n",
    "    # Save text data\n",
    "    text_data.to_csv(os.path.join(output_dir, 'processed_text.csv'), index=False)\n",
    "    \n",
    "    print(f\"Data saved to {output_dir}\")\n",
    "\n",
    "# Save the processed data\n",
    "print(\"\\nSaving processed data:\")\n",
    "save_data_splits(image_data, relevant_samples)\n",
    "\n",
    "# 4. Summary and Next Steps\n",
    "print(\"\\nData Preparation Summary:\")\n",
    "print(\"1. Processed X-ray images from existing train/val/test splits\")\n",
    "print(\"2. Identified relevant text samples from MTSamples\")\n",
    "print(\"3. Saved processed data for model training\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Train baseline models for each modality\")\n",
    "print(\"2. Design fusion architecture\")\n",
    "print(\"3. Evaluate combined model performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X-ray Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 409\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;66;03m# Main execution code\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# Create dataloaders\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset \u001b[38;5;241m=\u001b[39m create_dataloaders()\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     model \u001b[38;5;241m=\u001b[39m PneumoniaClassifier(base_model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdensenet121\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, freeze_base\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 188\u001b[0m, in \u001b[0;36mcreate_dataloaders\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m ChestXrayDataset(\n\u001b[1;32m    182\u001b[0m     csv_file\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    183\u001b[0m     transform\u001b[38;5;241m=\u001b[39mval_transform\n\u001b[1;32m    184\u001b[0m )\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Create dataloaders\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Setting num_workers=0 to avoid the multiprocessing issue\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    189\u001b[0m     train_dataset, \n\u001b[1;32m    190\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, \n\u001b[1;32m    191\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m    192\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    193\u001b[0m )\n\u001b[1;32m    195\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    196\u001b[0m     val_dataset, \n\u001b[1;32m    197\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, \n\u001b[1;32m    198\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m    199\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    200\u001b[0m )\n\u001b[1;32m    202\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    203\u001b[0m     test_dataset, \n\u001b[1;32m    204\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, \n\u001b[1;32m    205\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m    206\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    207\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:383\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 383\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m RandomSampler(dataset, generator\u001b[38;5;241m=\u001b[39mgenerator)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/sampler.py:165\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# Pneumonia Detection - PyTorch X-ray Model\n",
    "# Building a CNN model for pneumonia detection from chest X-rays using PyTorch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths to the processed data\n",
    "BASE_PATH = 'processed_data'\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# 1. Define the dataset class\n",
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"Dataset for chest X-ray images.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the csv file with image paths and labels.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data_frame.iloc[idx, 0]  # first column contains image paths\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        label = self.data_frame.iloc[idx, 1]  # second column contains labels\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "class RobustChestXrayDataset(Dataset):\n",
    "    \"\"\"Dataset for chest X-ray images with robustness against corrupted images.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None, fallback_image=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the csv file with image paths and labels.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            fallback_image (str, optional): Path to a fallback image to use for corrupted images.\n",
    "                If None, will use a random valid image from the dataset.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.fallback_image = fallback_image\n",
    "        self.valid_indices = list(range(len(self.data_frame)))\n",
    "        self.corrupted_images = set()\n",
    "        \n",
    "        # Check for corrupted images during initialization\n",
    "        self._validate_images()\n",
    "        \n",
    "        # Print dataset statistics\n",
    "        print(f\"Dataset initialized with {len(self.valid_indices)} valid images.\")\n",
    "        if self.corrupted_images:\n",
    "            print(f\"Found {len(self.corrupted_images)} corrupted images.\")\n",
    "    \n",
    "    def _validate_images(self):\n",
    "        \"\"\"Validate all images in the dataset and identify corrupted ones.\"\"\"\n",
    "        valid_images = []\n",
    "        self.corrupted_images = set()\n",
    "        \n",
    "        for idx in range(len(self.data_frame)):\n",
    "            img_path = self.data_frame.iloc[idx, 0]\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img.verify()  # Verify image is not corrupted\n",
    "                valid_images.append(idx)\n",
    "            except (UnidentifiedImageError, OSError, IOError) as e:\n",
    "                self.corrupted_images.add(idx)\n",
    "                logging.warning(f\"Corrupted image at index {idx}: {img_path}, Error: {e}\")\n",
    "        \n",
    "        self.valid_indices = valid_images\n",
    "        \n",
    "        # If no fallback image is provided, use the first valid image\n",
    "        if not self.fallback_image and valid_images:\n",
    "            self.fallback_image = self.data_frame.iloc[valid_images[0], 0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of valid images in the dataset.\"\"\"\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a sample from the dataset.\"\"\"\n",
    "        # Map the filtered index to the original index\n",
    "        mapped_idx = self.valid_indices[idx]\n",
    "        \n",
    "        img_path = self.data_frame.iloc[mapped_idx, 0]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except (UnidentifiedImageError, OSError, IOError) as e:\n",
    "            # This should not happen since we filtered out corrupted images\n",
    "            # But just in case, use the fallback image\n",
    "            logging.error(f\"Failed to load supposedly valid image: {img_path}, Error: {e}\")\n",
    "            if self.fallback_image:\n",
    "                image = Image.open(self.fallback_image).convert('RGB')\n",
    "            else:\n",
    "                # Create a blank image as last resort\n",
    "                image = Image.new('RGB', (224, 224), color=(128, 128, 128))\n",
    "        \n",
    "        label = self.data_frame.iloc[mapped_idx, 1]  # second column contains labels\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "# 2. Define data transformations\n",
    "def get_transforms():\n",
    "    \"\"\"Define data transformations for training and validation/testing.\"\"\"\n",
    "    # Training transformations with augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Validation/test transformations (no augmentation)\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "# 3. Create dataloaders\n",
    "def create_dataloaders():\n",
    "    \"\"\"Create PyTorch dataloaders for training, validation, and testing.\"\"\"\n",
    "    train_transform, val_transform = get_transforms()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ChestXrayDataset(\n",
    "        csv_file=os.path.join(BASE_PATH, 'train', 'image_data.csv'),\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = ChestXrayDataset(\n",
    "        csv_file=os.path.join(BASE_PATH, 'val', 'image_data.csv'),\n",
    "        transform=val_transform\n",
    "    )\n",
    "    \n",
    "    test_dataset = ChestXrayDataset(\n",
    "        csv_file=os.path.join(BASE_PATH, 'test', 'image_data.csv'),\n",
    "        transform=val_transform\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    # Setting num_workers=0 to avoid the multiprocessing issue\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Print dataset information\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# 4. Define the model using transfer learning\n",
    "class PneumoniaClassifier(nn.Module):\n",
    "    \"\"\"CNN model for pneumonia classification using transfer learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_name='densenet121', pretrained=True, freeze_base=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_model_name (str): Name of the base model to use.\n",
    "            pretrained (bool): Whether to use pretrained weights.\n",
    "            freeze_base (bool): Whether to freeze the base model layers.\n",
    "        \"\"\"\n",
    "        super(PneumoniaClassifier, self).__init__()\n",
    "        \n",
    "        # Load the base model\n",
    "        if base_model_name == 'densenet121':\n",
    "            self.base_model = models.densenet121(weights='IMAGENET1K_V1' if pretrained else None)\n",
    "            num_ftrs = self.base_model.classifier.in_features\n",
    "            self.base_model.classifier = nn.Identity()\n",
    "        elif base_model_name == 'resnet50':\n",
    "            self.base_model = models.resnet50(weights='IMAGENET1K_V1' if pretrained else None)\n",
    "            num_ftrs = self.base_model.fc.in_features\n",
    "            self.base_model.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported base model: {base_model_name}\")\n",
    "        \n",
    "        # Freeze the base model if requested\n",
    "        if freeze_base:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Custom classifier for pneumonia detection\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        self.base_model_name = base_model_name\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        features = self.base_model(x)\n",
    "        return self.classifier(features)\n",
    "    \n",
    "    def unfreeze_layers(self, num_layers=10):\n",
    "        \"\"\"Unfreeze the last num_layers of the base model for fine-tuning.\"\"\"\n",
    "        if self.base_model_name == 'densenet121':\n",
    "            layers = list(self.base_model.features.children())\n",
    "            for layer in layers[-num_layers:]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "        elif self.base_model_name == 'resnet50':\n",
    "            layers = list(self.base_model.children())\n",
    "            for layer in layers[-num_layers:]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "# 5. Define training and validation functions\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = torch.sigmoid(outputs) >= 0.5\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\"loss\": loss.item(), \"acc\": correct/total})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = torch.sigmoid(outputs) >= 0.5\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Store predictions and labels for metrics\n",
    "            all_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    # Calculate AUC\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    return epoch_loss, epoch_acc, roc_auc, all_preds, all_labels\n",
    "\n",
    "# 6. Training loop with early stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, \n",
    "                num_epochs=NUM_EPOCHS, patience=5):\n",
    "    \"\"\"Train and validate the model with early stopping.\"\"\"\n",
    "    best_val_auc = 0.0\n",
    "    best_model_wts = None\n",
    "    no_improve_epochs = 0\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_auc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc, val_auc, _, _ = validate(model, val_loader, criterion, device)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "        \n",
    "        # Step the scheduler if provided\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        # Check if this is the best model so far\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            no_improve_epochs = 0\n",
    "            print(f\"New best model with Val AUC: {val_auc:.4f}\")\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            print(f\"No improvement for {no_improve_epochs} epochs\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Main execution code\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = create_dataloaders()\n",
    "\n",
    "    # Initialize the model\n",
    "    model = PneumoniaClassifier(base_model_name='densenet121', pretrained=True, freeze_base=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model (first stage with frozen base layers)\n",
    "    print(\"Training Stage 1: Frozen base model\")\n",
    "    model, history_stage1 = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, None, num_epochs=10, patience=5\n",
    "    )\n",
    "\n",
    "    # Fine-tuning stage (unfreeze some layers)\n",
    "    print(\"\\nTraining Stage 2: Fine-tuning with unfrozen layers\")\n",
    "    model.unfreeze_layers(num_layers=10)  # Unfreeze the last 10 layers\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "\n",
    "    model, history_stage2 = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, None, num_epochs=10, patience=5\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'pneumonia_classifier.pth')\n",
    "    print(\"Model saved to pneumonia_classifier.pth\")\n",
    "\n",
    "    # 8. Plot training history\n",
    "    def plot_training_history(history_stage1, history_stage2=None):\n",
    "        \"\"\"Plot the training and validation metrics.\"\"\"\n",
    "        # Combine histories if we have two stages\n",
    "        if history_stage2:\n",
    "            train_loss = history_stage1['train_loss'] + history_stage2['train_loss']\n",
    "            val_loss = history_stage1['val_loss'] + history_stage2['val_loss']\n",
    "            train_acc = history_stage1['train_acc'] + history_stage2['train_acc']\n",
    "            val_acc = history_stage1['val_acc'] + history_stage2['val_acc']\n",
    "            val_auc = history_stage1['val_auc'] + history_stage2['val_auc']\n",
    "            epochs = range(1, len(train_loss) + 1)\n",
    "            stage1_end = len(history_stage1['train_loss'])\n",
    "        else:\n",
    "            train_loss = history_stage1['train_loss']\n",
    "            val_loss = history_stage1['val_loss']\n",
    "            train_acc = history_stage1['train_acc']\n",
    "            val_acc = history_stage1['val_acc']\n",
    "            val_auc = history_stage1['val_auc']\n",
    "            epochs = range(1, len(train_loss) + 1)\n",
    "            stage1_end = len(train_loss)\n",
    "        \n",
    "        # Create figure with 2 subplots\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
    "        plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "        if history_stage2:\n",
    "            plt.axvline(x=stage1_end, color='g', linestyle='--', label='Start Fine-tuning')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(epochs, train_acc, 'b-', label='Training Accuracy')\n",
    "        plt.plot(epochs, val_acc, 'r-', label='Validation Accuracy')\n",
    "        if history_stage2:\n",
    "            plt.axvline(x=stage1_end, color='g', linestyle='--', label='Start Fine-tuning')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot AUC\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(epochs, val_auc, 'r-', label='Validation AUC')\n",
    "        if history_stage2:\n",
    "            plt.axvline(x=stage1_end, color='g', linestyle='--', label='Start Fine-tuning')\n",
    "        plt.title('Validation AUC')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(history_stage1, history_stage2)\n",
    "\n",
    "    # 9. Evaluate on the test set\n",
    "    def evaluate_model(model, test_loader, criterion, device):\n",
    "        \"\"\"Evaluate the model on the test set.\"\"\"\n",
    "        # Get test predictions\n",
    "        test_loss, test_acc, test_auc, test_preds, test_labels = validate(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nTest Results:\")\n",
    "        print(f\"Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, AUC: {test_auc:.4f}\")\n",
    "        \n",
    "        # Convert predictions to binary\n",
    "        test_preds_binary = (np.array(test_preds) >= 0.5).astype(int)\n",
    "        \n",
    "        # Print classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(test_labels, test_preds_binary, target_names=['Normal', 'Pneumonia']))\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        cm = confusion_matrix(test_labels, test_preds_binary)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(\n",
    "            cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Pneumonia'], \n",
    "            yticklabels=['Normal', 'Pneumonia']\n",
    "        )\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        fpr, tpr, _ = roc_curve(test_labels, test_preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'loss': test_loss,\n",
    "            'accuracy': test_acc,\n",
    "            'auc': test_auc,\n",
    "            'predictions': test_preds,\n",
    "            'labels': test_labels\n",
    "        }\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_results = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "    # 10. Visualize some predictions\n",
    "    def visualize_predictions(model, test_dataset, num_samples=5):\n",
    "        \"\"\"Visualize some example predictions.\"\"\"\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Randomly select samples\n",
    "        indices = random.sample(range(len(test_dataset)), num_samples)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
    "        \n",
    "        # Get the inverse transform to display images\n",
    "        inv_normalize = transforms.Compose([\n",
    "            transforms.Normalize(\n",
    "                mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "                std=[1/0.229, 1/0.224, 1/0.225]\n",
    "            ),\n",
    "            transforms.ToPILImage()\n",
    "        ])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, idx in enumerate(indices):\n",
    "                # Get the image and label\n",
    "                image, label = test_dataset[idx]\n",
    "                image_tensor = image.unsqueeze(0).to(device)\n",
    "                \n",
    "                # Make prediction\n",
    "                output = model(image_tensor).squeeze()\n",
    "                pred_prob = torch.sigmoid(output).item()\n",
    "                pred_class = \"Pneumonia\" if pred_prob >= 0.5 else \"Normal\"\n",
    "                true_class = \"Pneumonia\" if label.item() == 1 else \"Normal\"\n",
    "                \n",
    "                # Color green for correct predictions, red for incorrect\n",
    "                color = \"green\" if pred_class == true_class else \"red\"\n",
    "                \n",
    "                # Convert tensor to image for display\n",
    "                img_to_show = inv_normalize(image.cpu())\n",
    "                \n",
    "                # Display the image and prediction\n",
    "                axes[i].imshow(img_to_show)\n",
    "                axes[i].set_title(f\"Pred: {pred_class}\\nTrue: {true_class}\\nProb: {pred_prob:.2f}\", color=color)\n",
    "                axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Visualize some predictions\n",
    "    visualize_predictions(model, test_dataset)\n",
    "\n",
    "    # 11. Extract and save features for multimodal fusion\n",
    "    def extract_features(model, dataloader, device):\n",
    "        \"\"\"Extract features from the model for the dataloader samples.\"\"\"\n",
    "        # Create a feature extractor model\n",
    "        if isinstance(model, PneumoniaClassifier):\n",
    "            # Create a copy of the model that outputs features before the final classification layer\n",
    "            class FeatureExtractor(nn.Module):\n",
    "                def __init__(self, model):\n",
    "                    super(FeatureExtractor, self).__init__()\n",
    "                    self.base_model = model.base_model\n",
    "                    \n",
    "                    # Get the feature extractor part (all but the last layer)\n",
    "                    classifier_layers = list(model.classifier.children())\n",
    "                    self.feature_layers = nn.Sequential(*classifier_layers[:-2])\n",
    "                \n",
    "                def forward(self, x):\n",
    "                    features = self.base_model(x)\n",
    "                    return self.feature_layers(features)\n",
    "            \n",
    "            feature_extractor = FeatureExtractor(model).to(device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model type for feature extraction\")\n",
    "        \n",
    "        # Extract features\n",
    "        feature_extractor.eval()\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, batch_labels in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "                inputs = inputs.to(device)\n",
    "                batch_features = feature_extractor(inputs)\n",
    "                features.append(batch_features.cpu().numpy())\n",
    "                labels.append(batch_labels.numpy())\n",
    "        \n",
    "        # Concatenate batches\n",
    "        features = np.vstack(features)\n",
    "        labels = np.concatenate(labels)\n",
    "        \n",
    "        return features, labels\n",
    "\n",
    "    # Extract features for all sets\n",
    "    print(\"\\nExtracting features for future multimodal fusion:\")\n",
    "    train_features, train_labels = extract_features(model, train_loader, device)\n",
    "    val_features, val_labels = extract_features(model, val_loader, device)\n",
    "    test_features, test_labels = extract_features(model, test_loader, device)\n",
    "\n",
    "    # Save features\n",
    "    features_path = 'image_features.npz'\n",
    "    np.savez(\n",
    "        features_path,\n",
    "        train_features=train_features, train_labels=train_labels,\n",
    "        val_features=val_features, val_labels=val_labels,\n",
    "        test_features=test_features, test_labels=test_labels\n",
    "    )\n",
    "    print(f\"Features saved to {features_path}\")\n",
    "\n",
    "    # 12. Summary and conclusion\n",
    "    print(\"\\nImage Model Summary:\")\n",
    "    print(f\"Architecture: DenseNet121 with custom classifier\")\n",
    "    print(f\"Training strategy: Two-stage training with transfer learning\")\n",
    "    print(f\"Test Accuracy: {test_results['accuracy']:.4f}\")\n",
    "    print(f\"Test AUC: {test_results['auc']:.4f}\")\n",
    "\n",
    "    print(\"\\nNext Steps:\")\n",
    "    print(\"1. Build a text model to extract key pneumonia indicators from medical text\")\n",
    "    print(\"2. Create a visualization dashboard showing X-ray predictions and text insights\")\n",
    "    print(\"3. Demonstrate how integrating both modalities improves diagnostic capability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders():\n",
    "    train_csv_path = os.path.join(BASE_PATH, 'train', 'image_data.csv')\n",
    "    val_csv_path = os.path.join(BASE_PATH, 'val', 'image_data.csv')\n",
    "    test_csv_path = os.path.join(BASE_PATH, 'test', 'image_data.csv')\n",
    "    \n",
    "    print(f\"Checking if train CSV exists: {os.path.exists(train_csv_path)}\")\n",
    "    if os.path.exists(train_csv_path):\n",
    "        df = pd.read_csv(train_csv_path)\n",
    "        print(f\"Train CSV shape: {df.shape}\")\n",
    "        print(f\"Train CSV first few rows:\\n{df.head()}\")\n",
    "    \n",
    "    # Rest of the function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking train split...\n",
      "Checking 0 images for corruption...\n",
      "No corrupted images found in train split.\n",
      "\n",
      "Checking val split...\n",
      "Checking 0 images for corruption...\n",
      "No corrupted images found in val split.\n",
      "\n",
      "Checking test split...\n",
      "Checking 0 images for corruption...\n",
      "No corrupted images found in test split.\n"
     ]
    }
   ],
   "source": [
    "# Pneumonia Detection - Error Handling for Corrupted Images\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import shutil\n",
    "\n",
    "def identify_corrupted_images(csv_file):\n",
    "    \"\"\"Check all images in the CSV file and identify any corrupted ones.\"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    corrupted_images = []\n",
    "    \n",
    "    print(f\"Checking {len(df)} images for corruption...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        img_path = row['image_path']\n",
    "        try:\n",
    "            # Try to open the image\n",
    "            with Image.open(img_path) as img:\n",
    "                # Try to load the image data\n",
    "                img.load()\n",
    "        except (UnidentifiedImageError, OSError, IOError) as e:\n",
    "            print(f\"Corrupted image found: {img_path}\")\n",
    "            print(f\"Error: {e}\")\n",
    "            corrupted_images.append(img_path)\n",
    "    \n",
    "    return corrupted_images\n",
    "\n",
    "def remove_corrupted_from_csv(csv_file, corrupted_images, output_file=None):\n",
    "    \"\"\"Remove corrupted images from the CSV file.\"\"\"\n",
    "    if not output_file:\n",
    "        output_file = csv_file.replace('.csv', '_clean.csv')\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Filter out corrupted images\n",
    "    clean_df = df[~df['image_path'].isin(corrupted_images)]\n",
    "    \n",
    "    # Save the clean dataframe\n",
    "    clean_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Removed {len(corrupted_images)} corrupted images from CSV.\")\n",
    "    print(f\"Original CSV had {len(df)} entries, new CSV has {len(clean_df)} entries.\")\n",
    "    print(f\"Clean CSV saved to {output_file}\")\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "def check_and_clean_dataset():\n",
    "    \"\"\"Check all splits of the dataset and clean them.\"\"\"\n",
    "    base_path = 'processed_data'\n",
    "    splits = ['train', 'val', 'test']\n",
    "    \n",
    "    for split in splits:\n",
    "        csv_path = os.path.join(base_path, split, 'image_data.csv')\n",
    "        \n",
    "        if os.path.exists(csv_path):\n",
    "            print(f\"\\nChecking {split} split...\")\n",
    "            corrupted = identify_corrupted_images(csv_path)\n",
    "            \n",
    "            if corrupted:\n",
    "                # Make backup of original CSV\n",
    "                backup_path = os.path.join(base_path, split, f'image_data_backup.csv')\n",
    "                shutil.copy(csv_path, backup_path)\n",
    "                print(f\"Backup created at {backup_path}\")\n",
    "                \n",
    "                # Remove corrupted images from CSV\n",
    "                remove_corrupted_from_csv(csv_path, corrupted, csv_path)\n",
    "            else:\n",
    "                print(f\"No corrupted images found in {split} split.\")\n",
    "        else:\n",
    "            print(f\"CSV file not found for {split} split: {csv_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_and_clean_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
